# -*- coding: utf8 -*-
"""
Description: Auxiliary class with semantic similarity measuresw
Created in January 2017
@author: Dagmar Gromann
"""

import numpy as np

from gensim import utils, matutils
from math import exp
from numpy.linalg import norm
from scipy.stats import entropy
from sklearn.preprocessing import Normalizer

def gauss_s(x1, x2, d):
    sigma = 1
    return np.exp(-(d(x1, x2))/(2*sigma**2))
    
def cos_s(x1, x2, d):
    return -(dist.cosine(x1, x2) - 1)

def JensenShanon(M):
    Matrix = []
    for x in M:  
        vector = np.zeros(len(M))
        counter = -1
        for y in M:
            counter += 1
            vector[counter] = exp(-JSD(x, y))
        Matrix.append(vector)
    M = np.array(Matrix)
    return M

def JSD(P, Q):
    _P = P / norm(P, ord=1)
    _Q = Q / norm(Q, ord=1)
    _M = 0.5 * (_P + _Q)
    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))

#takes an M matrix generated by get_M and returns a tf_idf matrix
def get_tf_idf_M(M, tf = ["bin", "raw", "log", "dnorm"], idf = ["c", "smooth", "max", "prob"], norm_samps=False):
    N = len(M)
    if tf == "raw":
        tf_M = np.copy(M) #just the frequency of the word in a text
    if idf == "c":
        idf_v = []
        #get the number of texts that contain a word words[i]
        for i in range(M.shape[1]):
            #count the non zero values in columns of matrix M
            idf_v.append(np.count_nonzero(M[:,i])) 
        idf_v = np.array(idf_v)
        idf_v = np.log(N/idf_v)
    tf_idf_M = tf_M*idf_v
    if norm_samps:
        normalizer = Normalizer()
        tf_idf_M = normalizer.fit_transform(tf_idf_M)
    return tf_idf_M

'''
Convert the raw frequencies to Positive Pointwise Mutual Information (PPMI) values
'''
def raw2ppmi(cooccur, k_shift=1.0):
    # following lines a bit tedious, as we try to avoid making temporary copies of the (large) `cooccur` matrix
    marginal_word = cooccur.sum(axis=1)
    marginal_context = cooccur.sum(axis=0)
    cooccur /= marginal_word[:, None]  
    cooccur /= marginal_context 
    cooccur *= marginal_word.sum() 
    np.log(cooccur, out=cooccur) 

    #Shfiting PMI scores by log(k)
    cooccur -= np.log(k_shift) 

    #Clipping values to be non-negative
    cooccur.clip(0.0, out=cooccur)

    #Normalize PPMI vectors to unit length
    for i, vec in enumerate(cooccur):
        cooccur[i] = matutils.unitvec(vec)

    return cooccur
